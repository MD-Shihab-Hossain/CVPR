{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8747a01d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'Python 3.14.0' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Three Hidden Layer Neural Network for Multi-Class Classification\n",
    "Assignment Implementation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "class MultiClassNeuralNetwork:\n",
    "    \"\"\"\n",
    "    Neural Network with three hidden layers for multi-class classification.\n",
    "    \n",
    "    Architecture:\n",
    "    - Input Layer: n_features neurons\n",
    "    - Hidden Layer 1: h1_neurons neurons\n",
    "    - Hidden Layer 2: h2_neurons neurons  \n",
    "    - Hidden Layer 3: h3_neurons neurons\n",
    "    - Output Layer: n_classes neurons (softmax activation)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, h1_size=64, h2_size=32, h3_size=16, output_size=5, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initialize the neural network with random weights and biases.\n",
    "        \n",
    "        Parameters:\n",
    "        - input_size: Number of input features\n",
    "        - h1_size: Number of neurons in first hidden layer\n",
    "        - h2_size: Number of neurons in second hidden layer\n",
    "        - h3_size: Number of neurons in third hidden layer\n",
    "        - output_size: Number of output classes\n",
    "        - learning_rate: Learning rate for gradient descent\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize weights using He initialization for better convergence\n",
    "        self.W1 = np.random.randn(input_size, h1_size) * np.sqrt(2.0 / input_size)\n",
    "        self.b1 = np.zeros((1, h1_size))\n",
    "        \n",
    "        self.W2 = np.random.randn(h1_size, h2_size) * np.sqrt(2.0 / h1_size)\n",
    "        self.b2 = np.zeros((1, h2_size))\n",
    "        \n",
    "        self.W3 = np.random.randn(h2_size, h3_size) * np.sqrt(2.0 / h2_size)\n",
    "        self.b3 = np.zeros((1, h3_size))\n",
    "        \n",
    "        self.W4 = np.random.randn(h3_size, output_size) * np.sqrt(2.0 / h3_size)\n",
    "        self.b4 = np.zeros((1, output_size))\n",
    "        \n",
    "    def relu(self, x):\n",
    "        \"\"\"ReLU activation function: f(x) = max(0, x)\"\"\"\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        \"\"\"Derivative of ReLU: f'(x) = 1 if x > 0, else 0\"\"\"\n",
    "        return (x > 0).astype(float)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        Softmax activation function for multi-class classification.\n",
    "        Converts logits to probability distribution.\n",
    "        \n",
    "        softmax(x_i) = exp(x_i) / sum(exp(x_j)) for all j\n",
    "        \"\"\"\n",
    "        # Subtract max for numerical stability\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Returns all intermediate activations for use in backpropagation.\n",
    "        \"\"\"\n",
    "        # Input to Hidden Layer 1\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.relu(self.z1)\n",
    "        \n",
    "        # Hidden Layer 1 to Hidden Layer 2\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.relu(self.z2)\n",
    "        \n",
    "        # Hidden Layer 2 to Hidden Layer 3\n",
    "        self.z3 = np.dot(self.a2, self.W3) + self.b3\n",
    "        self.a3 = self.relu(self.z3)\n",
    "        \n",
    "        # Hidden Layer 3 to Output Layer\n",
    "        self.z4 = np.dot(self.a3, self.W4) + self.b4\n",
    "        self.a4 = self.softmax(self.z4)\n",
    "        \n",
    "        return self.a4\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute categorical cross-entropy loss.\n",
    "        \n",
    "        Loss = -sum(y_true * log(y_pred)) / n_samples\n",
    "        \"\"\"\n",
    "        m = y_true.shape[0]\n",
    "        # Add small epsilon to prevent log(0)\n",
    "        log_likelihood = -np.log(y_pred[range(m), y_true] + 1e-8)\n",
    "        loss = np.sum(log_likelihood) / m\n",
    "        return loss\n",
    "    \n",
    "    def backward_propagation(self, X, y_true):\n",
    "        \"\"\"\n",
    "        Backward pass to compute gradients using backpropagation.\n",
    "        \n",
    "        Uses chain rule to compute gradients layer by layer from output to input.\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Convert y_true to one-hot encoding\n",
    "        y_one_hot = np.zeros_like(self.a4)\n",
    "        y_one_hot[range(m), y_true] = 1\n",
    "        \n",
    "        # Output layer gradient\n",
    "        # For softmax + cross-entropy: dL/dz4 = a4 - y_true\n",
    "        dz4 = self.a4 - y_one_hot\n",
    "        dW4 = np.dot(self.a3.T, dz4) / m\n",
    "        db4 = np.sum(dz4, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Hidden layer 3 gradient\n",
    "        da3 = np.dot(dz4, self.W4.T)\n",
    "        dz3 = da3 * self.relu_derivative(self.z3)\n",
    "        dW3 = np.dot(self.a2.T, dz3) / m\n",
    "        db3 = np.sum(dz3, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Hidden layer 2 gradient\n",
    "        da2 = np.dot(dz3, self.W3.T)\n",
    "        dz2 = da2 * self.relu_derivative(self.z2)\n",
    "        dW2 = np.dot(self.a1.T, dz2) / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Hidden layer 1 gradient\n",
    "        da1 = np.dot(dz2, self.W2.T)\n",
    "        dz1 = da1 * self.relu_derivative(self.z1)\n",
    "        dW1 = np.dot(X.T, dz1) / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Update weights and biases using gradient descent\n",
    "        self.W4 -= self.learning_rate * dW4\n",
    "        self.b4 -= self.learning_rate * db4\n",
    "        \n",
    "        self.W3 -= self.learning_rate * dW3\n",
    "        self.b3 -= self.learning_rate * db3\n",
    "        \n",
    "        self.W2 -= self.learning_rate * dW2\n",
    "        self.b2 -= self.learning_rate * db2\n",
    "        \n",
    "        self.W1 -= self.learning_rate * dW1\n",
    "        self.b1 -= self.learning_rate * db1\n",
    "    \n",
    "    def train(self, X, y, epochs=1000, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the neural network using gradient descent.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Training features\n",
    "        - y: Training labels\n",
    "        - epochs: Number of training iterations\n",
    "        - verbose: Whether to print training progress\n",
    "        \n",
    "        Returns:\n",
    "        - history: Dictionary containing loss and accuracy per epoch\n",
    "        \"\"\"\n",
    "        history = {'loss': [], 'accuracy': []}\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward propagation\n",
    "            y_pred = self.forward_propagation(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.compute_loss(y, y_pred)\n",
    "            \n",
    "            # Backward propagation\n",
    "            self.backward_propagation(X, y)\n",
    "            \n",
    "            # Compute accuracy\n",
    "            predictions = np.argmax(y_pred, axis=1)\n",
    "            accuracy = np.mean(predictions == y)\n",
    "            \n",
    "            # Store metrics\n",
    "            history['loss'].append(loss)\n",
    "            history['accuracy'].append(accuracy)\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and (epoch % 100 == 0 or epoch == epochs - 1):\n",
    "                print(f\"Epoch {epoch}/{epochs} - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on new data.\n",
    "        \n",
    "        Returns:\n",
    "        - Class predictions (integers from 0 to n_classes-1)\n",
    "        \"\"\"\n",
    "        y_pred = self.forward_propagation(X)\n",
    "        return np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Get probability distributions for predictions.\n",
    "        \n",
    "        Returns:\n",
    "        - Probability matrix of shape (n_samples, n_classes)\n",
    "        \"\"\"\n",
    "        return self.forward_propagation(X)\n",
    "\n",
    "\n",
    "def generate_synthetic_dataset(n_samples=1000, n_features=10, n_classes=5):\n",
    "    \"\"\"\n",
    "    Generate synthetic multi-class classification dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - n_samples: Number of samples to generate\n",
    "    - n_features: Number of input features\n",
    "    - n_classes: Number of output classes\n",
    "    \n",
    "    Returns:\n",
    "    - X: Feature matrix\n",
    "    - y: Label vector\n",
    "    \"\"\"\n",
    "    print(f\"\\nGenerating synthetic dataset...\")\n",
    "    print(f\"- Samples: {n_samples}\")\n",
    "    print(f\"- Features: {n_features}\")\n",
    "    print(f\"- Classes: {n_classes}\")\n",
    "    \n",
    "    X, y = make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=n_features,\n",
    "        n_informative=8,\n",
    "        n_redundant=2,\n",
    "        n_classes=n_classes,\n",
    "        n_clusters_per_class=1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Normalize features to [0, 1] range\n",
    "    X = (X - X.min()) / (X.max() - X.min())\n",
    "    \n",
    "    print(f\"Dataset generated successfully!\")\n",
    "    print(f\"- Feature shape: {X.shape}\")\n",
    "    print(f\"- Label shape: {y.shape}\")\n",
    "    print(f\"- Class distribution: {np.bincount(y)}\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate the trained model on test data.\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary containing various evaluation metrics\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average=None, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average=None, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average=None, zero_division=0)\n",
    "    \n",
    "    # Overall metrics (macro average)\n",
    "    precision_macro = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    recall_macro = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"\\nMacro-averaged Metrics:\")\n",
    "    print(f\"- Precision: {precision_macro:.4f}\")\n",
    "    print(f\"- Recall: {recall_macro:.4f}\")\n",
    "    print(f\"- F1-Score: {f1_macro:.4f}\")\n",
    "    \n",
    "    print(f\"\\nPer-Class Metrics:\")\n",
    "    print(f\"{'Class':<8} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}\")\n",
    "    print(\"-\" * 48)\n",
    "    for i in range(len(precision)):\n",
    "        print(f\"{i:<8} {precision[i]:<12.4f} {recall[i]:<12.4f} {f1[i]:<12.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': cm,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training loss and accuracy over epochs.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    ax1.plot(history['loss'], linewidth=2)\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Loss', fontsize=12)\n",
    "    ax1.set_title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax2.plot(history['accuracy'], linewidth=2, color='green')\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Accuracy', fontsize=12)\n",
    "    ax2.set_title('Training Accuracy Over Time', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"\\nTraining history plot saved as 'training_history.png'\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, n_classes=5):\n",
    "    \"\"\"Plot confusion matrix as a heatmap.\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=range(n_classes),\n",
    "                yticklabels=range(n_classes),\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.xlabel('Predicted Class', fontsize=12)\n",
    "    plt.ylabel('True Class', fontsize=12)\n",
    "    plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"Confusion matrix plot saved as 'confusion_matrix.png'\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_metrics_comparison(metrics):\n",
    "    \"\"\"Plot per-class metrics comparison.\"\"\"\n",
    "    n_classes = len(metrics['precision'])\n",
    "    classes = [f'Class {i}' for i in range(n_classes)]\n",
    "    \n",
    "    x = np.arange(n_classes)\n",
    "    width = 0.25\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.bar(x - width, metrics['precision'], width, label='Precision', alpha=0.8)\n",
    "    ax.bar(x, metrics['recall'], width, label='Recall', alpha=0.8)\n",
    "    ax.bar(x + width, metrics['f1_score'], width, label='F1-Score', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Classes', fontsize=12)\n",
    "    ax.set_ylabel('Score', fontsize=12)\n",
    "    ax.set_title('Per-Class Performance Metrics', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(classes)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.set_ylim([0, 1.1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"Metrics comparison plot saved as 'metrics_comparison.png'\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the complete assignment workflow.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"THREE HIDDEN LAYER NEURAL NETWORK\")\n",
    "    print(\"Multi-Class Classification Assignment\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Step 1: Generate synthetic dataset\n",
    "    X, y = generate_synthetic_dataset(n_samples=1000, n_features=10, n_classes=5)\n",
    "    \n",
    "    # Step 2: Split dataset into training and testing sets\n",
    "    print(\"\\nSplitting dataset into train (80%) and test (20%) sets...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    print(f\"Training set size: {X_train.shape[0]}\")\n",
    "    print(f\"Testing set size: {X_test.shape[0]}\")\n",
    "    \n",
    "    # Step 3: Initialize neural network\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"NETWORK ARCHITECTURE\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Input Layer: 10 neurons\")\n",
    "    print(\"Hidden Layer 1: 64 neurons (ReLU activation)\")\n",
    "    print(\"Hidden Layer 2: 32 neurons (ReLU activation)\")\n",
    "    print(\"Hidden Layer 3: 16 neurons (ReLU activation)\")\n",
    "    print(\"Output Layer: 5 neurons (Softmax activation)\")\n",
    "    print(\"Learning Rate: 0.01\")\n",
    "    \n",
    "    model = MultiClassNeuralNetwork(\n",
    "        input_size=10,\n",
    "        h1_size=64,\n",
    "        h2_size=32,\n",
    "        h3_size=16,\n",
    "        output_size=5,\n",
    "        learning_rate=0.01\n",
    "    )\n",
    "    \n",
    "    # Step 4: Train the model\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    history = model.train(X_train, y_train, epochs=1000, verbose=True)\n",
    "    \n",
    "    # Step 5: Evaluate the model\n",
    "    metrics = evaluate_model(model, X_test, y_test)\n",
    "    \n",
    "    # Step 6: Visualize results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"VISUALIZATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    plot_training_history(history)\n",
    "    plot_confusion_matrix(metrics['confusion_matrix'])\n",
    "    plot_metrics_comparison(metrics)\n",
    "    \n",
    "    # Step 7: Final summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nFinal Test Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(\"\\nKey Observations:\")\n",
    "    print(\"1. The network successfully learned to classify 5 different classes\")\n",
    "    print(\"2. Three hidden layers allow the network to learn complex decision boundaries\")\n",
    "    print(\"3. ReLU activation helps prevent vanishing gradient problem\")\n",
    "    print(\"4. Softmax output provides probability distributions for multi-class classification\")\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
